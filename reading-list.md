## My Reading List of Papers

### Autonomous Cars:

1. End to End Learning for Self-Driving Cars. *Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba*. arXiv1604.07316 (2016)
    * **url**: [https://arxiv.org/pdf/1604.07316.pdf](https://arxiv.org/pdf/1604.07316.pdf)

### Convolutional Neural Networks:

1. Gradient-based learning applied to document recognition. *Y. Lecun, L. Bottou, Y. Bengio, P. Haffner*. Proc. IEEE 86 (2002) 2278-2324
    * **doi**: [10.1109/5.726791](10.1109/5.726791)
    * **url**: [http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)

### ImageNet Challenges:

*Winners of large scale visiual recognition challenges*
1. Identity Mappings in Deep Residual Networks. *Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun*. arXiv1603.05027 (2016)
    * **url**: [https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)
    * **comment**: modified architecture of ResNet.

2. Deep Residual Learning for Image Recognition. *Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun*. arXiv1512.03385 (2015)
    * **url**: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)
    * **comment**: 2015 winner, ResNet. 0.036 top 5 error. presentation link: http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf

3. Very Deep Convolutional Networks for Large-Scale Image Recognition. *Karen Simonyan, Andrew Zisserman*. arXiv1409.1556 (2014)
    * **url**: [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)
    * **comment**: 2014 VGGNet. 0.073 top 5 error.

4. Going Deeper with Convolutions. *Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich*. arXiv1409.4842 (2014)
    * **url**: [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)
    * **comment**: 2014 winner, GoogleNet. 0.067 top 5 error.

5. Visualizing and Understanding Convolutional Networks. *Matthew D Zeiler, Rob Fergus*. arXiv1311.2901 (2013)
    * **url**: [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)
    * **comment**: 2013, 0.148 error.

6. ImageNet Classification with Deep Convolutional Neural Networks. *Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton*. NIPS (2012)
    * **url**: [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)
    * **comment**: AlexNet, 0.154 top 5 error. Community noticed the success of deep convolutional networks at large scale.

### Residual Networks:

1. Deep Networks with Stochastic Depth. *Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger*. arXiv1603.09382 (2016)
    * **url**: [https://arxiv.org/abs/1603.09382](https://arxiv.org/abs/1603.09382)
    * **comment**: Randomization of layers in ResNets.

### it from bit:

1. Self-organized criticality: An explanation of the 1/fnoise. *Per Bak, Chao Tang, Kurt Wiesenfeld*. Phys. Rev. Lett. 59 (2002) 381-384
    * **doi**: [https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.59.381](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.59.381)
    * **url**: [http://www-liphy.ujf-grenoble.fr/pagesperso/barrat/sc/bak2.pdf](http://www-liphy.ujf-grenoble.fr/pagesperso/barrat/sc/bak2.pdf)
    * **comment**: About self-organized criticality.

### Generative Adversarial Networks:

1. NIPS 2016 Tutorial: Generative Adversarial Networks. *Ian Goodfellow*. arXiv1701.00160 (2016)
    * **url**: [https://arxiv.org/pdf/1701.00160v1.pdf](https://arxiv.org/pdf/1701.00160v1.pdf)

### Memory in NNs:

1. Overcoming catastrophic forgetting in neural networks. *James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, Raia Hadsell*. Proc Natl Acad Sci USA (2017) 201611835
    * **doi**: [10.1073/pnas.1611835114](10.1073/pnas.1611835114)
    * **comment**: Deepmind, elastic weight consolidation.

### Evolution Strategies:

*A Scalable Alternative to RL.*
1. Evolution Strategies as a Scalable Alternative to Reinforcement Learning. *Tim Salimans, Jonathan Ho, Xi Chen, Ilya Sutskever*. arXiv1703.03864 (2017)
    * **url**: [https://arxiv.org/abs/1703.03864](https://arxiv.org/abs/1703.03864)

### NNs Model Improvements:

1. Exploiting Cyclic Symmetry in Convolutional Neural Networks. *Sander Dieleman, Jeffrey De Fauw, Koray Kavukcuoglu*. arXiv1602.02660 (2016)
    * **url**: [https://arxiv.org/pdf/1602.02660.pdf](https://arxiv.org/pdf/1602.02660.pdf)
    * **comment**: Rotational-invariance.

2. Improving neural networks by preventing co-adaptation of feature
  detectors. *Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov*. arXiv1207.0580 (2012)
    * **url**: [https://arxiv.org/pdf/1207.0580.pdf](https://arxiv.org/pdf/1207.0580.pdf)
    * **comment**: Drop out technique to improve optimization: "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This “overfitting” is greatly reduced by randomly omitting half of the feature detectors on each training case."


-
*Created with [papermark](https://github.com/wphw/papermark/).*